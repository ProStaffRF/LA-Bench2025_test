{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrV21bF8yUQL",
        "outputId": "391eaf81-fb8e-49dd-87f1-e28f5d9b6a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Google ColaboratoryÁí∞Â¢É„ÇíÊ§úÂá∫„Åó„Åæ„Åó„Åü\n",
            "\n",
            "üì¶ ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´‰∏≠...\n",
            "‚úÖ „É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´ÂÆå‰∫Ü\n",
            "\n",
            "üì• „É™„Éù„Ç∏„Éà„É™„Çí„ÇØ„É≠„Éº„É≥‰∏≠: https://github.com/lasa-or-jp/la-bench.git\n",
            "‚úÖ „É™„Éù„Ç∏„Éà„É™„ÅÆ„ÇØ„É≠„Éº„É≥ÂÆå‰∫Ü: la-bench/\n",
            "\n",
            "üìç ‰ΩúÊ•≠„Éá„Ç£„É¨„ÇØ„Éà„É™: /content/la-bench\n",
            "\n",
            "üìä „Éó„É≠„Ç∏„Çß„ÇØ„ÉàÊßãÈÄ†:\n",
            "total 420\n",
            "drwxr-xr-x 9 root root   4096 Nov 19 05:08 .\n",
            "drwxr-xr-x 1 root root   4096 Nov 19 05:07 ..\n",
            "drwxr-xr-x 2 root root   4096 Nov 19 05:08 announcements\n",
            "-rw-r--r-- 1 root root   4741 Nov 19 05:08 CLAUDE.md\n",
            "drwxr-xr-x 4 root root   4096 Nov 19 05:08 code\n",
            "-rw-r--r-- 1 root root   2658 Nov 19 05:08 CONTRIBUTING.md\n",
            "drwxr-xr-x 5 root root   4096 Nov 19 05:08 data\n",
            "drwxr-xr-x 3 root root   4096 Nov 19 05:08 docs\n",
            "drwxr-xr-x 8 root root   4096 Nov 19 05:08 .git\n",
            "drwxr-xr-x 3 root root   4096 Nov 19 05:08 .github\n",
            "-rw-r--r-- 1 root root   1086 Nov 19 05:08 .gitignore\n",
            "-rw-r--r-- 1 root root   1101 Nov 19 05:08 LICENSE\n",
            "drwxr-xr-x 2 root root   4096 Nov 19 05:08 notebooks\n",
            "-rw-r--r-- 1 root root    569 Nov 19 05:08 pyproject.toml\n",
            "-rw-r--r-- 1 root root  13396 Nov 19 05:08 README.md\n",
            "-rw-r--r-- 1 root root    121 Nov 19 05:08 requirements.txt\n",
            "-rw-r--r-- 1 root root 346546 Nov 19 05:08 uv.lock\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Áí∞Â¢ÉË®≠ÂÆö„Å®„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó\n",
        "\"\"\"\n",
        "LA-Bench 2025: ÂÆüÈ®ìÊâãÈ†ÜÁîüÊàê„Çø„Çπ„ÇØ\n",
        "Baseline Implementation for Google Colaboratory\n",
        "GitHub: https://github.com/lasa-or-jp/la-bench.git\n",
        "\"\"\"\n",
        "\n",
        "#@title 1. Áí∞Â¢É„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó { display-mode: \"form\" }\n",
        "#@markdown „Åì„ÅÆ„Çª„É´„ÇíÂÆüË°å„Åó„Å¶ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„ÄÅ„É™„Éù„Ç∏„Éà„É™„Çí„ÇØ„É≠„Éº„É≥„Åó„Åæ„Åô„ÄÇ\n",
        "\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Colab„Åã„Å©„ÅÜ„Åã„ÅÆÁ¢∫Ë™ç\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"‚úÖ Google ColaboratoryÁí∞Â¢É„ÇíÊ§úÂá∫„Åó„Åæ„Åó„Åü\")\n",
        "    # ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n",
        "    print(\"\\nüì¶ ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´‰∏≠...\")\n",
        "    !pip install -q openai pyyaml tqdm pandas\n",
        "\n",
        "    print(\"‚úÖ „É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´ÂÆå‰∫Ü\")\n",
        "\n",
        "    # GitHub„É™„Éù„Ç∏„Éà„É™„ÅÆ„ÇØ„É≠„Éº„É≥\n",
        "    REPO_URL = \"https://github.com/lasa-or-jp/la-bench.git\"\n",
        "    REPO_NAME = \"la-bench\"\n",
        "\n",
        "    if not os.path.exists(REPO_NAME):\n",
        "        print(f\"\\nüì• „É™„Éù„Ç∏„Éà„É™„Çí„ÇØ„É≠„Éº„É≥‰∏≠: {REPO_URL}\")\n",
        "        !git clone -q {REPO_URL}\n",
        "        print(f\"‚úÖ „É™„Éù„Ç∏„Éà„É™„ÅÆ„ÇØ„É≠„Éº„É≥ÂÆå‰∫Ü: {REPO_NAME}/\")\n",
        "    else:\n",
        "        print(f\"\\nüìÇ „É™„Éù„Ç∏„Éà„É™„ÅØÊó¢„Å´Â≠òÂú®„Åó„Åæ„Åô: {REPO_NAME}/\")\n",
        "        print(\"üì• ÊúÄÊñ∞Áâà„Å´Êõ¥Êñ∞‰∏≠...\")\n",
        "        !cd {REPO_NAME} && git pull -q\n",
        "        print(\"‚úÖ Êõ¥Êñ∞ÂÆå‰∫Ü\")\n",
        "\n",
        "    # ‰ΩúÊ•≠„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆË®≠ÂÆö\n",
        "    WORK_DIR = Path(REPO_NAME)\n",
        "    os.chdir(WORK_DIR)\n",
        "    print(f\"\\nüìç ‰ΩúÊ•≠„Éá„Ç£„É¨„ÇØ„Éà„É™: {os.getcwd()}\")\n",
        "\n",
        "    # „Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†„ÅÆÁ¢∫Ë™ç\n",
        "    print(\"\\nüìä „Éó„É≠„Ç∏„Çß„ÇØ„ÉàÊßãÈÄ†:\")\n",
        "    !ls -la\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚ö†Ô∏è „É≠„Éº„Ç´„É´Áí∞Â¢É„ÅßÂÆüË°å‰∏≠„Åß„Åô\")\n",
        "    if Path.cwd().name == \"notebooks\":\n",
        "        os.chdir(Path.cwd().parent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PZr_6rXsyqlK",
        "outputId": "77ef2a8b-2114-424c-b906-80e7ada6f2c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë OpenAI API Key„ÇíÂÖ•Âäõ: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ API„Ç≠„Éº„ÅåË®≠ÂÆö„Åï„Çå„Åæ„Åó„Åü\n",
            "üîë API„Ç≠„Éº: ********************Ct8A\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: OpenAI API„Ç≠„Éº„ÅÆË®≠ÂÆö\n",
        "#@title 2. OpenAI API KeyË®≠ÂÆö { display-mode: \"form\" }\n",
        "#@markdown OpenAI API„Ç≠„Éº„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Ç≠„Éº„ÅØÂÆâÂÖ®„Å´ÁÆ°ÁêÜ„Åï„Çå„Åæ„Åô„ÄÇ\n",
        "\n",
        "\n",
        "# API„Ç≠„Éº„ÅÆÂèñÂæóÊñπÊ≥ï„ÇíÈÅ∏Êäû\n",
        "use_secrets = False  #@param {type:\"boolean\"}\n",
        "#@markdown ‚òùÔ∏è Google Colab Secrets„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÅØ„ÉÅ„Çß„ÉÉ„ÇØ\n",
        "\n",
        "if IN_COLAB:\n",
        "    import getpass\n",
        "    from google.colab import userdata\n",
        "    if use_secrets:\n",
        "        try:\n",
        "            # Colab Secrets„Åã„ÇâÂèñÂæó\n",
        "            API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "            print(\"‚úÖ API„Ç≠„Éº„ÇíSecrets„Åã„ÇâÂèñÂæó„Åó„Åæ„Åó„Åü\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Secrets„Åã„Çâ„ÅÆÂèñÂæó„Å´Â§±Êïó„Åó„Åæ„Åó„Åü\")\n",
        "            print(\"Â∑¶ÂÅ¥„ÅÆ„Éë„Éç„É´„ÅÆüîë„Ç¢„Ç§„Ç≥„É≥„Åã„Çâ'OPENAI_API_KEY'„ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ\")\n",
        "            API_KEY = None\n",
        "    else:\n",
        "        # Áõ¥Êé•ÂÖ•Âäõ\n",
        "        api_key_input = getpass.getpass(\"üîë OpenAI API Key„ÇíÂÖ•Âäõ: \")\n",
        "        if api_key_input:\n",
        "            API_KEY = api_key_input\n",
        "            os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "            print(\"‚úÖ API„Ç≠„Éº„ÅåË®≠ÂÆö„Åï„Çå„Åæ„Åó„Åü\")\n",
        "        else:\n",
        "            API_KEY = None\n",
        "            print(\"‚ö†Ô∏è API„Ç≠„Éº„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„ÇìÔºà„Éí„É•„Éº„É™„Çπ„ÉÜ„Ç£„ÉÉ„ÇØÊâãÊ≥ï„ÅÆ„Åø‰ΩøÁî®Ôºâ\")\n",
        "else:\n",
        "    # „É≠„Éº„Ç´„É´Áí∞Â¢É„ÅÆÂ†¥Âêà\n",
        "    API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not API_KEY:\n",
        "        API_KEY = input(\"OpenAI API Key: \")\n",
        "\n",
        "# API„Ç≠„Éº„ÅÆÊ§úË®º\n",
        "if API_KEY:\n",
        "    print(f\"üîë API„Ç≠„Éº: {'*' * 20}{API_KEY[-4:]}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPTÊ©üËÉΩ„ÅØ‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "71dPwmdnzEfP",
        "outputId": "15686dc0-4512-41d1-ae61-74b238cd0959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LA-Bench 2025 Baseline Implementation\n",
            "ÂÆüË°åÁí∞Â¢É: Google Colab\n",
            "ÂÆüË°åÊôÇÂàª: 2025-11-19 05:16:58\n",
            "OpenAIÂà©Áî®ÂèØËÉΩ: True\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: „É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Éù„Éº„Éà„Å®Ë®≠ÂÆö\n",
        "#@title 3. „É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Éù„Éº„Éà { display-mode: \"form\" }\n",
        "\n",
        "import json\n",
        "import yaml\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple, Set, Any\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# „Éá„Éº„ÇøÂá¶ÁêÜ\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# OpenAI API\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    OPENAI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPENAI_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è OpenAI„É©„Ç§„Éñ„É©„É™„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì\")\n",
        "\n",
        "# „Éó„É≠„Ç∞„É¨„Çπ„Éê„Éº (ColabÂØæÂøú)\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# „É≠„Ç∞Ë®≠ÂÆö\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LA-Bench 2025 Baseline Implementation\")\n",
        "print(f\"ÂÆüË°åÁí∞Â¢É: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
        "print(f\"ÂÆüË°åÊôÇÂàª: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"OpenAIÂà©Áî®ÂèØËÉΩ: {OPENAI_AVAILABLE and API_KEY is not None}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0rypmgB20Yc2"
      },
      "outputs": [],
      "source": [
        "# Cell 4: „Éá„Éº„ÇøÊßãÈÄ†\n",
        "#@title 4. „Éá„Éº„ÇøÊßãÈÄ†„ÅÆÂÆöÁæ© { display-mode: \"form\" }\n",
        "\n",
        "@dataclass\n",
        "class Step:\n",
        "    id: int\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class ReferenceEntry:\n",
        "    id: int\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class ExampleInput:\n",
        "    instruction: str\n",
        "    mandatory_objects: Set[str] = field(default_factory=set)\n",
        "    source_protocol_steps: List[Step] = field(default_factory=list)\n",
        "    expected_final_states: Set[str] = field(default_factory=set)\n",
        "    references: List[ReferenceEntry] = field(default_factory=list)\n",
        "\n",
        "@dataclass\n",
        "class ExampleOutput:\n",
        "    procedure_steps: List[Step] = field(default_factory=list)\n",
        "\n",
        "@dataclass\n",
        "class Measurement:\n",
        "    specific_criteria: Dict[str, int] = field(default_factory=dict)\n",
        "\n",
        "@dataclass\n",
        "class ExampleSample:\n",
        "    id: str\n",
        "    input: ExampleInput\n",
        "    output: ExampleOutput\n",
        "    measurement: Optional[Measurement] = None\n",
        "\n",
        "def _to_set(x):\n",
        "    return set(x) if isinstance(x, (list, set, tuple)) else set()\n",
        "\n",
        "def _to_list(x):\n",
        "    return list(x) if isinstance(x, (list, set, tuple)) else (x if isinstance(x, list) else [])\n",
        "\n",
        "def _to_steps(x) -> List[Step]:\n",
        "    steps: List[Step] = []\n",
        "    arr = _to_list(x)\n",
        "    if not arr:\n",
        "        return steps\n",
        "    if isinstance(arr[0], dict):\n",
        "        for it in arr:\n",
        "            try:\n",
        "                sid = int(it.get(\"id\", len(steps) + 1))\n",
        "            except Exception:\n",
        "                sid = len(steps) + 1\n",
        "            steps.append(Step(id=sid, text=str(it.get(\"text\", \"\")).strip()))\n",
        "    else:\n",
        "        for idx, s in enumerate(arr, start=1):\n",
        "            steps.append(Step(id=idx, text=str(s).strip()))\n",
        "    return steps\n",
        "\n",
        "def _to_references(x) -> List[ReferenceEntry]:\n",
        "    refs: List[ReferenceEntry] = []\n",
        "    arr = _to_list(x)\n",
        "    if not arr:\n",
        "        return refs\n",
        "    if isinstance(arr[0], dict):\n",
        "        for it in arr:\n",
        "            try:\n",
        "                rid = int(it.get(\"id\", len(refs) + 1))\n",
        "            except Exception:\n",
        "                rid = len(refs) + 1\n",
        "            refs.append(ReferenceEntry(id=rid, text=str(it.get(\"text\", \"\")).strip()))\n",
        "    else:\n",
        "        for idx, ref in enumerate(arr, start=1):\n",
        "            refs.append(ReferenceEntry(id=idx, text=str(ref).strip()))\n",
        "    return refs\n",
        "\n",
        "def parse_sample(obj: Dict[str, Any]) -> ExampleSample:\n",
        "    sid = obj.get(\"id\") or obj.get(\"sample_id\") or \"unknown\"\n",
        "    i = obj.get(\"input\", {})\n",
        "    o = obj.get(\"output\", {})\n",
        "    m = obj.get(\"measurement\", {})\n",
        "\n",
        "    # Measurement.specific_criteria „Çí dict „Å´Ê≠£Ë¶èÂåñÔºàlistÂΩ¢Âºè„ÇÇË®±ÂÆπÔºâ\n",
        "    sc_raw = m.get(\"specific_criteria\", {})\n",
        "    sc: Dict[str, int] = {}\n",
        "    if isinstance(sc_raw, dict):\n",
        "        for k, v in sc_raw.items():\n",
        "            try:\n",
        "                sc[str(k)] = int(v)\n",
        "            except Exception:\n",
        "                pass\n",
        "    elif isinstance(sc_raw, list):\n",
        "        for it in sc_raw:\n",
        "            try:\n",
        "                k = it.get(\"item\")\n",
        "                v = int(it.get(\"score\", 0))\n",
        "                if k:\n",
        "                    sc[str(k)] = v\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    sample = ExampleSample(\n",
        "        id=str(sid),\n",
        "        input=ExampleInput(\n",
        "            instruction=str(i.get(\"instruction\", \"\")).strip(),\n",
        "            mandatory_objects=_to_set(i.get(\"mandatory_objects\", [])),\n",
        "            source_protocol_steps=_to_steps(i.get(\"source_protocol_steps\", [])),\n",
        "            expected_final_states=_to_set(i.get(\"expected_final_states\", [])),\n",
        "            references=_to_references(i.get(\"references\", [])),\n",
        "        ),\n",
        "        output=ExampleOutput(\n",
        "            procedure_steps=_to_steps(o.get(\"procedure_steps\", []))\n",
        "        ),\n",
        "        measurement=Measurement(specific_criteria=sc) if sc else None\n",
        "    )\n",
        "    return sample\n",
        "\n",
        "def load_example_jsonl(path: str):\n",
        "    samples = []\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"JSONL not found: {p}\")\n",
        "    for line in p.read_text(encoding=\"utf-8\").splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è JSONL parse error: {e}\")\n",
        "            continue\n",
        "        samples.append(parse_sample(obj))\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MXF0Ou0B6sFB",
        "outputId": "41541882-093b-40f6-a100-b11d1483f26c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 10 samples from data/public_test/public_test.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: JSONL„É≠„Éº„ÉÄ„Éº„ÅÆÂà©Áî®\n",
        "#@title 5. JSONL„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ { display-mode: \"form\" }\n",
        "#@markdown example„Çí‰Ωø„ÅÜ„Å®„ÅçÔºö`data/example/example.jsonl`\n",
        "#@markdown public_test„Çí‰Ωø„ÅÜ„Å®„ÅçÔºö`data/public_test/public_test.jsonl`\n",
        "\n",
        "jsonl_path = 'data/public_test/public_test.jsonl'  #@param {type:'string'}\n",
        "\n",
        "try:\n",
        "    samples = load_example_jsonl(jsonl_path)\n",
        "    print(f'‚úÖ Loaded {len(samples)} samples from {jsonl_path}')\n",
        "except Exception as e:\n",
        "    print(f'‚ùå Load error: {e}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WZ94EpzOInJr",
        "outputId": "47bd81a8-0b00-423c-f6da-0197e0075f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ÁîüÊàêÂÆå‰∫Ü: 10 samples\n",
            "‰æã: public_test_1 ‚Üí 16 steps\n",
            "üìÑ Saved JSONL: outputs/runs/generated_20251119_054439.jsonl\n",
            "Download file: outputs/runs/generated_20251119_054439.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f55728d6-c5a9-4fd1-b6d6-b7491e1006b2\", \"generated_20251119_054439.jsonl\", 21430)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Cell 6: ÂÆüÈ®ìÊâãÈ†Ü„ÅÆÁîüÊàêÔºàOpenAI, PydanticÊßãÈÄ†ÂåñÔºâ\n",
        "#@title 6. LLM„Åß Input „Åã„Çâ OutputÔºàprocedure_stepsÔºâ„ÇíÁîüÊàê { display-mode: \"form\" }\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# „É¢„Éá„É´Ë®≠ÂÆö\n",
        "MODEL_NAME = \"gpt-4o-2024-08-06\" #@param [\"gpt-4.1-mini-2025-04-14\", \"gpt-4o-2024-08-06\", \"gpt-5-2025-08-07\", \"gpt-5-mini-2025-08-07\", \"gpt-5-nano-2025-08-07\"]\n",
        "#@markdown gpt-4o-mini, gpt-4o-2024-08-06, „ÅÇ„Çã„ÅÑ„ÅØ„Åù„Çå‰ª•Èôç„ÅÆ„É¢„Éá„É´„Å´ÂØæÂøú„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ<br>\n",
        "#@markdown (Structured output„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ) <br>\n",
        "#@markdown gpt-5Á≥ª„É¢„Éá„É´„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÄÅtemperature=1.0„Å®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
        "TEMPERATURE = 0.7 # @param\n",
        "\n",
        "#@markdown `build_messages`Èñ¢Êï∞„Å´„Åä„ÅÑ„Å¶„ÄÅLLM„ÅÆÂÖ•Âäõ„ÇíË®≠Ë®à„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n",
        "\n",
        "class StepModel(BaseModel):\n",
        "    id: int = Field(ge=1, description=\"„Çπ„ÉÜ„ÉÉ„ÉóÁï™Âè∑\")\n",
        "    text: str = Field(description=\"ÂÆüÈ®ìÊâãÈ†Ü„ÅÆË©≥Á¥∞„Å™Ë™¨Êòé\")\n",
        "\n",
        "class GeneratedOutput(BaseModel):\n",
        "    procedure_steps: List[StepModel] = Field(\n",
        "        description=\"ÂÆüÈ®ìÊâãÈ†Ü„ÅÆ„É™„Çπ„Éà\",\n",
        "        min_items=1,\n",
        "        max_items=50\n",
        "    )\n",
        "\n",
        "def build_messages(sample: ExampleSample) -> list[dict]:\n",
        "    sys = (\n",
        "        \"„ÅÇ„Å™„Åü„ÅØ„ÄÅÂåñÂ≠¶„ÄÅÁîüÁâ©Â≠¶„ÄÅ„Åä„Çà„Å≥ÊùêÊñôÁßëÂ≠¶„ÇíÂê´„ÇÄÂÖ®ÂàÜÈáé„ÅÆ**„É©„Éú„Ç™„Éº„Éà„É°„Éº„Ç∑„Éß„É≥ (LA) „Éó„É≠„Éà„Ç≥„É´Â§âÊèõ„ÅÆÊúÄÈ´òÂ∞ÇÈñÄÂÆ∂**„Åß„Åô„ÄÇ\"\n",
        "        \"„ÅÇ„Å™„Åü„ÅÆÂîØ‰∏Ä„ÅÆ‰ªªÂãô„ÅØ„ÄÅÊèê‰æõ„Åï„Çå„Åü Input „ÇíË™≠„ÅøËæº„Åø„ÄÅLA„Ç∑„Çπ„ÉÜ„É†„ÅåÂÆüË°åÂèØËÉΩ„Å™**Ê•µ„ÇÅ„Å¶Âé≥Ê†º„ÅßË©≥Á¥∞„Å™ÂÆüÈ®ìÊâãÈ†Ü**Ôºàprocedure_stepsÔºâ„ÇíËøî„Åô„Åì„Å®„Åß„Åô„ÄÇ\"\n",
        "        \"ÊúÄÈáçË¶ÅÂà∂Á¥Ñ: Âá∫Âäõ„ÅØ„ÄÅPydantic„Çπ„Ç≠„Éº„Éû„Å´Âé≥Ê†º„Å´Ê∫ñÊã†„Åó„Åü**JSON„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆ„Åø**„Åß„ÅÇ„Çä„ÄÅ„ÅÑ„Åã„Å™„ÇãË™¨Êòé„ÇÑ„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÇÇË®±„Åï„Çå„Åæ„Åõ„Çì„ÄÇ\"\n",
        "        \"Âà∂Á¥Ñ: „Çπ„ÉÜ„ÉÉ„ÉóÊï∞„ÅØÊúÄÂ§ß50„ÄÅÂêÑ„Çπ„ÉÜ„ÉÉ„Éó„ÅØ10Êñá‰ª•‰∏ã„ÄÅid„ÅØ1„Åã„ÇâÊòáÈ†Ü„Åß„Å™„Åë„Çå„Å∞„Å™„Çä„Åæ„Åõ„Çì„ÄÇ\"\n",
        "    )\n",
        "    user_lines = []\n",
        "    user_lines.append(f\"„ÄêÂÆüÈ®ìÊåáÁ§∫„Äë\\n{sample.input.instruction}\")\n",
        "    if sample.input.mandatory_objects:\n",
        "        user_lines.append(\"\\n„Äê‰ΩøÁî®„Åô„ÇãÁâ©ÂìÅ„Äë\")\n",
        "        for it in sorted(sample.input.mandatory_objects):\n",
        "            user_lines.append(f\"- {it}\")\n",
        "    if sample.input.source_protocol_steps:\n",
        "        user_lines.append(\"\\n„ÄêÂÖÉ„Éó„É≠„Éà„Ç≥„É´„ÅÆÊâãÈ†ÜÔºàÂèÇËÄÉÔºâ„Äë\")\n",
        "        for st in sample.input.source_protocol_steps:\n",
        "            user_lines.append(f\"- {st.id}. {st.text}\")\n",
        "    if sample.input.expected_final_states:\n",
        "        user_lines.append(\"\\n„ÄêÊúüÂæÖ„Åï„Çå„ÇãÊúÄÁµÇÁä∂ÊÖã„Äë\")\n",
        "        for fs in sorted(sample.input.expected_final_states):\n",
        "            user_lines.append(f\"- {fs}\")\n",
        "    if sample.input.references:\n",
        "        user_lines.append(\"\\n„ÄêÂèÇËÄÉÊñáÁåÆ„Äë\")\n",
        "        for ref in sample.input.references:\n",
        "            user_lines.append(f\"- [{ref.id}] {ref.text}\")\n",
        "    usr = \"\\n\".join(user_lines)\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": sys},\n",
        "        {\"role\": \"user\", \"content\": usr},\n",
        "    ]\n",
        "\n",
        "def generate_outputs(samples: list[ExampleSample]) -> list[dict]:\n",
        "    client = OpenAI(api_key=API_KEY)\n",
        "    results: list[dict] = []\n",
        "    for sm in samples:\n",
        "        msgs = build_messages(sm)\n",
        "        try:\n",
        "            completion = client.chat.completions.parse(\n",
        "                model=MODEL_NAME,\n",
        "                messages=msgs,\n",
        "                temperature=TEMPERATURE,\n",
        "                response_format=GeneratedOutput,\n",
        "            )\n",
        "            parsed: GeneratedOutput = completion.choices[0].message.parsed  # type: ignore\n",
        "            steps = [\n",
        "                Step(id=s.id, text=s.text)\n",
        "                for s in sorted(parsed.procedure_steps, key=lambda x: x.id)\n",
        "            ][:50]\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ÁîüÊàêÂ§±Êïó: {sm.id}: {e}\")\n",
        "            steps = []  # no fallback\n",
        "        results.append({\n",
        "            \"id\": sm.id,\n",
        "            \"procedure_steps\": [{\"id\": s.id, \"text\": s.text} for s in steps],\n",
        "        })\n",
        "    print(f\"‚úÖ ÁîüÊàêÂÆå‰∫Ü: {len(results)} samples\")\n",
        "    return results\n",
        "\n",
        "# ÂÆüË°å\n",
        "generated_results = generate_outputs(samples)\n",
        "if generated_results:\n",
        "    print(f\"‰æã: {generated_results[0]['id']} ‚Üí {len(generated_results[0]['procedure_steps'])} steps\")\n",
        "\n",
        "# ÁîüÊàêÁµêÊûú„Çí JSONL „Åß‰øùÂ≠ò„Åó„ÄÅ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„É™„É≥„ÇØ„ÇíË°®Á§∫\n",
        "ts = time.strftime('%Y%m%d_%H%M%S')\n",
        "out_dir = Path('./outputs/runs')\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "jsonl_path = out_dir / f'generated_{ts}.jsonl'\n",
        "with jsonl_path.open('w', encoding='utf-8') as f:\n",
        "    for rec in generated_results:\n",
        "        obj = {\"id\": rec[\"id\"], \"output\": {\"procedure_steps\": rec[\"procedure_steps\"]}}\n",
        "        line = json.dumps(obj, ensure_ascii=False, separators=(\",\", \":\"))\n",
        "        f.write(line + \"\\n\")\n",
        "print(f\"üìÑ Saved JSONL: {jsonl_path}\")\n",
        "\n",
        "# „ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÔºàColab/„É≠„Éº„Ç´„É´ÂèåÊñπ„Å´ÂØæÂøúÔºâ\n",
        "try:\n",
        "    from google.colab import files as colab_files  # type: ignore\n",
        "    # „ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÁ¢∫Ë™ç„ÉÄ„Ç§„Ç¢„É≠„Ç∞„ÇíÂá∫„Åó„Å¶„ÄÅy„Å™„Çâ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ\n",
        "    from google.colab.output import eval_js\n",
        "    print(f\"Download file: {jsonl_path}\")\n",
        "    confirm = eval_js('confirm(\"ÁîüÊàê„Åï„Çå„ÅüJSONL„Éï„Ç°„Ç§„É´„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÅãÔºü\")')\n",
        "    if confirm:\n",
        "      colab_files.download(str(jsonl_path))\n",
        "    else:\n",
        "      print(\"„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åó„Åü„ÄÇ\")\n",
        "\n",
        "except Exception:\n",
        "    from IPython.display import FileLink, display\n",
        "    display(FileLink(str(jsonl_path.resolve())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYtMf3bJOzcN",
        "outputId": "65dee2e4-97b2-4222-deb0-d77eb3e98a41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17:47:06 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "17:47:10 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "17:47:15 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "17:47:19 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "17:47:26 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM-as-a-judge: Scored 5 samples (0-10)\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "id",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "general_score",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "specific_score",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "total_score",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "4041fe37-63be-488d-a078-e15eb18feb7e",
              "rows": [
                [
                  "0",
                  "sample_1",
                  "5.0",
                  "3.0",
                  "8.0"
                ],
                [
                  "1",
                  "sample_2",
                  "5.0",
                  "3.0",
                  "8.0"
                ],
                [
                  "2",
                  "sample_3",
                  "5.0",
                  "0.0",
                  "5.0"
                ],
                [
                  "3",
                  "sample_4",
                  "5.0",
                  "3.0",
                  "8.0"
                ],
                [
                  "4",
                  "sample_5",
                  "5.0",
                  "4.0",
                  "9.0"
                ]
              ],
              "shape": {
                "columns": 4,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>general_score</th>\n",
              "      <th>specific_score</th>\n",
              "      <th>total_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sample_1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sample_2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sample_3</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sample_4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sample_5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  general_score  specific_score  total_score\n",
              "0  sample_1            5.0             3.0          8.0\n",
              "1  sample_2            5.0             3.0          8.0\n",
              "2  sample_3            5.0             0.0          5.0\n",
              "3  sample_4            5.0             3.0          8.0\n",
              "4  sample_5            5.0             4.0          9.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Saved: outputs/runs/eval_llm_20251011_174658.csv\n",
            "Download file: outputs/runs/eval_llm_20251011_174658.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<a href='/Users/kato/Dropbox/Programs/gensurv/la-bench/outputs/runs/eval_llm_20251011_174658.csv' target='_blank'>/Users/kato/Dropbox/Programs/gensurv/la-bench/outputs/runs/eval_llm_20251011_174658.csv</a><br>"
            ],
            "text/plain": [
              "/Users/kato/Dropbox/Programs/gensurv/la-bench/outputs/runs/eval_llm_20251011_174658.csv"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Cell 7: LLM-as-a-judge Ë©ï‰æ°Ôºà10ÁÇπÊ∫ÄÁÇπÔºâ\n",
        "#@title 7. LLM „ÅßÂÖ±ÈÄö5ÁÇπ + ÂÄãÂà•5ÁÇπ„ÇíÊé°ÁÇπ { display-mode: \"form\" }\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"OpenAI SDK v1 „ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ`uv add openai` „ÅßËøΩÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\") from e\n",
        "\n",
        "JUDGE_MODEL = \"gpt-4.1-mini\"  # È´òÊÄßËÉΩÊé®Â•®„É¢„Éá„É´„Å´Â§âÊõ¥ÂèØ\n",
        "JUDGE_TEMPERATURE = 0.2\n",
        "\n",
        "class JudgeOutput(BaseModel):\n",
        "    general_score: float = Field(ge=0, le=5)\n",
        "    specific_score: float = Field(ge=0, le=5)\n",
        "    final_score: float = Field(ge=0, le=10)\n",
        "    general_reason: str\n",
        "    specific_matches: List[str] = []\n",
        "    notes: Optional[str] = None\n",
        "\n",
        "def build_judge_messages(sample: ExampleSample, steps: List[Step]) -> list[dict]:\n",
        "    # Ë©ï‰æ°Âü∫Ê∫ñÔºàÂÖ±ÈÄö5ÁÇπ + ÂÄãÂà•5ÁÇπÔºâ\n",
        "    system = (\n",
        "        \"„ÅÇ„Å™„Åü„ÅØÁîüÂëΩÁßëÂ≠¶ÂÆüÈ®ì„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„ÅÇ„Çä„ÄÅÂÖ¨Âπ≥„Å™Êé°ÁÇπËÄÖ„Åß„Åô„ÄÇ\"\n",
        "        \"‰ª•‰∏ã„ÅÆÂü∫Ê∫ñ„Å´Âæì„Å£„Å¶„ÄÅ‰∏é„Åà„Çâ„Çå„Åü Input „Å®ÁîüÊàêÊâãÈ†ÜÔºàOutputÔºâ„ÇíË©ï‰æ°„Åó„ÄÅ\"\n",
        "        \"general_score(0-5) „Å® specific_score(0-5) „Å® final_score(0-10) „ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\"\n",
        "        \"\\n\\n[ÂÖ±ÈÄöÊé°ÁÇπÂü∫Ê∫ñ 5ÁÇπÊ∫ÄÁÇπ]\\n\"\n",
        "        \"Âä†ÁÇπ(+1„Åö„Å§): 1) ÂÆüÈ®ìÊåáÁ§∫„ÅÆ„Éë„É©„É°„Éº„ÇøÂèçÊò†, 2) ‰ΩøÁî®„Åô„ÇãÁâ©ÂìÅ„ÅÆÂèçÊò†, 3) ÂÖÉÊâãÈ†Ü„ÅÆË´ñÁêÜÂèçÊò†, 4) ÊúüÂæÖ„Åï„Çå„ÇãÊúÄÁµÇÁä∂ÊÖã„ÅÆÈÅîÊàê, 5) ÈÅ©Âàá„Å™Ë£úÂÆå„ÄÇ\\n\"\n",
        "        \"Ê∏õÁÇπ: ‰∏çËá™ÁÑ∂„Å™Êó•Êú¨Ë™û/„Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥, Ë®àÁÆó„Éü„Çπ, ÊâãÈ†ÜÁüõÁõæ„ÄÇ\\n\"\n",
        "        \"‰∏äÈôê: ÂÖ•ÂäõÊâãÈ†Ü„ÅÆ‰∏∏ÂÜô„ÅóÁ≠â„ÅÆÈÅéÂ∫¶„ÅÆÂÆâÂÖ®ÊÄß„ÅåË¶ã„Çâ„Çå„ÇãÂ†¥Âêà„ÄÅgeneral_score „ÅØÊúÄÂ§ß2ÁÇπ„Å´Âà∂Èôê„ÄÇ\\n\\n\"\n",
        "        \"[ÂÄãÂà•Êé°ÁÇπÂü∫Ê∫ñ 5ÁÇπÊ∫ÄÁÇπ]\\n\"\n",
        "        \"‰∏é„Åà„Çâ„Çå„Åü specific_criteria „ÅÆÂêÑ item „ÅåÊâãÈ†Ü„Å´Âê´„Åæ„Çå„Çã/Ê∫Ä„Åü„Åô„Å™„Çâ„ÄÅ„Åù„ÅÆ score „ÇíÂä†ÁÇπÔºàÂêàË®à5ÁÇπ„Åß‰∏äÈôêÔºâ„ÄÇ\"\n",
        "    )\n",
        "\n",
        "    parts = []\n",
        "    parts.append(f\"„ÄêÂÆüÈ®ìÊåáÁ§∫„Äë\\n{sample.input.instruction}\")\n",
        "    if sample.input.mandatory_objects:\n",
        "        parts.append(\"\\n„Äê‰ΩøÁî®„Åô„ÇãÁâ©ÂìÅ„Äë\")\n",
        "        for it in sorted(sample.input.mandatory_objects):\n",
        "            parts.append(f\"- {it}\")\n",
        "    if sample.input.source_protocol_steps:\n",
        "        parts.append(\"\\n„ÄêÂÖÉ„Éó„É≠„Éà„Ç≥„É´„ÅÆÊâãÈ†ÜÔºàÂèÇËÄÉÔºâ„Äë\")\n",
        "        for st in sample.input.source_protocol_steps:\n",
        "            parts.append(f\"- {st.id}. {st.text}\")\n",
        "    if sample.input.expected_final_states:\n",
        "        parts.append(\"\\n„ÄêÊúüÂæÖ„Åï„Çå„ÇãÊúÄÁµÇÁä∂ÊÖã„Äë\")\n",
        "        for fs in sorted(sample.input.expected_final_states):\n",
        "            parts.append(f\"- {fs}\")\n",
        "    if sample.input.references:\n",
        "        parts.append(\"\\n„ÄêÂèÇËÄÉÊñáÁåÆ„Äë\")\n",
        "        for ref in sample.input.references:\n",
        "            parts.append(f\"- [{ref.id}] {ref.text}\")\n",
        "\n",
        "    parts.append(\"\\n„ÄêÁîüÊàêÊâãÈ†ÜÔºàOutputÔºâ„Äë\")\n",
        "    for s in steps:\n",
        "        parts.append(f\"- {s.id}. {s.text}\")\n",
        "\n",
        "    parts.append(\"\\n„Äêspecific_criteria„Äë\")\n",
        "    if sample.measurement and sample.measurement.specific_criteria:\n",
        "        for item, sc in sample.measurement.specific_criteria.items():\n",
        "            parts.append(f\"- ({int(sc)}ÁÇπ) {item}\")\n",
        "    else:\n",
        "        parts.append(\"- „Å™„Åó\")\n",
        "\n",
        "    user = \"\\n\".join(parts)\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": user},\n",
        "    ]\n",
        "\n",
        "def judge_with_llm(samples: List[ExampleSample], generated: list[dict]) -> pd.DataFrame:\n",
        "    client = OpenAI(api_key=API_KEY) if 'API_KEY' in globals() and API_KEY else OpenAI()\n",
        "    proc_map = {g['id']: [Step(id=it['id'], text=it['text']) for it in g['procedure_steps']] for g in generated}\n",
        "    rows = []\n",
        "    quota_exhausted = False\n",
        "    def _is_insufficient_quota(err: Exception) -> bool:\n",
        "        s = str(err)\n",
        "        return 'insufficient_quota' in s or 'You exceeded your current quota' in s\n",
        "    for sm in samples:\n",
        "        if quota_exhausted:\n",
        "            print(f\"‚è≠Ô∏è „Çπ„Ç≠„ÉÉ„ÉóÊé°ÁÇπ: {sm.id}Ôºà„ÇØ„Ç©„Éº„Çø‰∏çË∂≥Ôºâ\")\n",
        "            rows.append({\n",
        "                'id': sm.id,\n",
        "                'general_score': 0.0,\n",
        "                'specific_score': 0.0,\n",
        "                'total_score': 0.0,\n",
        "                'notes': 'skipped_due_to_quota',\n",
        "            })\n",
        "            continue\n",
        "        steps = proc_map.get(sm.id, [])\n",
        "        msgs = build_judge_messages(sm, steps)\n",
        "        try:\n",
        "            completion = client.chat.completions.parse(\n",
        "                model=JUDGE_MODEL,\n",
        "                messages=msgs,\n",
        "                temperature=JUDGE_TEMPERATURE,\n",
        "                response_format=JudgeOutput,\n",
        "            )\n",
        "            parsed: JudgeOutput = completion.choices[0].message.parsed  # type: ignore\n",
        "            rows.append({\n",
        "                'id': sm.id,\n",
        "                'general_score': parsed.general_score,\n",
        "                'specific_score': parsed.specific_score,\n",
        "                'total_score': parsed.final_score,\n",
        "                'notes': parsed.notes or '',\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Ë©ï‰æ°Â§±Êïó: {sm.id}: {e}\")\n",
        "            if _is_insufficient_quota(e):\n",
        "                print(\"‚ö†Ô∏è API„ÇØ„Ç©„Éº„Çø‰∏çË∂≥„ÅÆ„Åü„ÇÅ„ÄÅ‰ª•Èôç„ÅÆÊé°ÁÇπ„Çí‰∏≠Êñ≠„Åó„Åæ„Åô„ÄÇ„Éó„É©„É≥/Ë™≤ÈáëË®≠ÂÆö„Çí„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ„ÄÇ\")\n",
        "                quota_exhausted = True\n",
        "            rows.append({\n",
        "                'id': sm.id,\n",
        "                'general_score': 0.0,\n",
        "                'specific_score': 0.0,\n",
        "                'total_score': 0.0,\n",
        "                'notes': 'evaluation_failed',\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ÂÆüË°å\n",
        "df = judge_with_llm(samples, generated_results)\n",
        "print(f\"‚úÖ LLM-as-a-judge: Scored {len(df)} samples (0-10)\")\n",
        "try:\n",
        "    display(df[['id','general_score','specific_score','total_score']])\n",
        "except Exception:\n",
        "    print(df[['id','general_score','specific_score','total_score']])\n",
        "\n",
        "csv_path = out_dir / f'eval_llm_{ts}.csv'\n",
        "df.to_csv(csv_path, index=False, encoding=\"utf_8_sig\")\n",
        "print(f'üìÑ Saved: {csv_path}')\n",
        "\n",
        "try:\n",
        "    # „ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÁ¢∫Ë™ç„ÉÄ„Ç§„Ç¢„É≠„Ç∞„ÇíÂá∫„Åó„Å¶„ÄÅy„Å™„Çâ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ\n",
        "    print(f\"Download file: {csv_path}\")\n",
        "    confirm = eval_js('confirm(\"ÁîüÊàê„Åï„Çå„ÅüCSV„Éï„Ç°„Ç§„É´„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÅãÔºü\")')\n",
        "    if confirm:\n",
        "      colab_files.download(str(csv_path))\n",
        "    else:\n",
        "      print(\"„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åó„Åü„ÄÇ\")\n",
        "\n",
        "except Exception:\n",
        "    display(FileLink(str(csv_path.resolve())))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "la-bench",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}